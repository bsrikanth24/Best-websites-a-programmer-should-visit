{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKnXaRofDUHHuPUWpU+UEg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsrikanth24/Best-websites-a-programmer-should-visit/blob/master/Pyspark_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Write a Spark code snippet to calculate the sum of a column in a DataFrame**"
      ],
      "metadata": {
        "id": "jaCagYwQfnn-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "VTKJJ0lTdiJR",
        "outputId": "1e308143-a93b-4173-85c7-0992b4881589"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Name: string, email: string, salary: double]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
        "\n",
        "data = [(\"Srikanth\", \"john@example.com\", 50000.0),\n",
        "    (\"Ram\", \"Ram@example.com\", 60000.0),\n",
        "    (\"Venkat\", \"Venkat@example.com\", 55000.0)]\n",
        "\n",
        "schema=\"Name string,email string,salary double\"\n",
        "df=spark.createDataFrame(data,schema)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,sum\n",
        "df_final=df.agg(sum(col(\"salary\")).alias(\"total_salary\"))\n",
        "# df_final=df.agg(sum(col(\"salary\")).alias(\"total_salary\")).first()[0]\n",
        "# df_final\n",
        "\n",
        "row_object = df_final.first()\n",
        "print(row_object)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm1PnZ_Bd-EA",
        "outputId": "f0763be7-9597-43d5-d76f-4a4ded0a5013"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(total_salary=165000.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Get all the Marks for individual in list**"
      ],
      "metadata": {
        "id": "Zxqi_U10fv4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data=[(1,'Srikanth',34),(1,'Modi',40),(1,'Modi',34),(2,'Shah',45),(2,'Shah',50)]\n",
        "# schema=\"ID int,Name string,Marks int\"\n",
        "# df=spark.createDataFrame(data,schema)\n",
        "\n",
        "# from pyspark.sql.functions import collect_list,collect_set,col\n",
        "\n",
        "# # df_final=df.groupBy(col(\"ID\"),col(\"Name\")).agg(collect_list(col('Marks')))\n",
        "# df_final = df.groupBy(col(\"ID\"), col(\"Name\")).agg(collect_list(col('Marks')))\n",
        "# display(df_final)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import collect_list, col\n",
        "\n",
        "data = [(1,'Srikanth',29),(1,'Modi',40),(1,'Modi',34),(2,'Shah',45),(2,'Shah',50)]\n",
        "schema = \"ID int, Name string, Marks int\"\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "df_final = (df.groupBy(\"ID\", \"Name\")\n",
        "             .agg(collect_list(\"Marks\").alias(\"Marks_List\")))\n",
        "\n",
        "df_final.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVVv-38Yf2e9",
        "outputId": "c81b8892-3841-4989-808f-e4262e0da181"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+\n",
            "|ID |Name    |Marks_List|\n",
            "+---+--------+----------+\n",
            "|1  |Modi    |[40, 34]  |\n",
            "|1  |Srikanth|[29]      |\n",
            "|2  |Shah    |[45, 50]  |\n",
            "+---+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.**Identify rows containing non-numeric values in the “Quantity” column, if any**"
      ],
      "metadata": {
        "id": "NIMdSTVLhvhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "schema = StructType([\n",
        "  StructField(\"ProductCode\", StringType(), True),\n",
        "  StructField(\"Quantity\", StringType(), True),\n",
        "  StructField(\"UnitPrice\", StringType(), True),\n",
        "  StructField(\"CustomerID\", StringType(), True),\n",
        "])\n",
        "\n",
        "data = [\n",
        "  (\"Q001\", 5, 20.0, \"C001\"),\n",
        "  (\"Q002\", 3, 15.5, \"C002\"),\n",
        "  (\"Q003\", 10, 5.99, \"C003\"),\n",
        "  (\"Q004\", 2, 50.0, \"C001\"),\n",
        "  (\"Q005\", \"nan\", 12.75, \"C002\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i839iJw0hyeO",
        "outputId": "8d305ec2-f0b6-4417-c1ce-df1f30d7d556"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+---------+----------+\n",
            "|ProductCode|Quantity|UnitPrice|CustomerID|\n",
            "+-----------+--------+---------+----------+\n",
            "|       Q001|       5|     20.0|      C001|\n",
            "|       Q002|       3|     15.5|      C002|\n",
            "|       Q003|      10|     5.99|      C003|\n",
            "|       Q004|       2|     50.0|      C001|\n",
            "|       Q005|     nan|    12.75|      C002|\n",
            "+-----------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "df_final=df.filter(col(\"Quantity\").rlike('^[a-zA-Z]*$'))\n",
        "df_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6onmJQFiChF",
        "outputId": "fab3af0d-e915-4edb-8143-42f326ac32c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+---------+----------+\n",
            "|ProductCode|Quantity|UnitPrice|CustomerID|\n",
            "+-----------+--------+---------+----------+\n",
            "|       Q005|     nan|    12.75|      C002|\n",
            "+-----------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write PySpark Code to get the below output**"
      ],
      "metadata": {
        "id": "WuQ2kgV4iWHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, split, explode_outer, trim\n",
        "\n",
        "# Create the input DataFrame\n",
        "data = [('India', 'Hockey, Cricket'), ('UAE', 'Soccer, Fight'), ('Sam', None)]\n",
        "schema = \"Name string, Sports string\"\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# df_final = df.filter(col(\"Hobbies\").isNotNull()) \\\n",
        "#               .select(\"Name\", explode(split(col(\"Hobbies\"), \",\")).alias(\"col\"))\n",
        "\n",
        "# df_final.show(truncate=False)\n",
        "\n",
        "df_final = (\n",
        "    df.withColumn(\"Sports\", split(col(\"Sports\"), \",\"))\n",
        "    .select(\"Name\", explode_outer(col(\"Sports\")).alias(\"col\"))\n",
        "    .withColumn(\"col\", trim(col(\"col\"))))\n",
        "\n",
        "df_final.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ft4FHzQiYw1",
        "outputId": "b10c6f16-82cd-4ebc-b11a-b8df70ed9853"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+\n",
            "|Name |col    |\n",
            "+-----+-------+\n",
            "|India|Hockey |\n",
            "|India|Cricket|\n",
            "|UAE  |Soccer |\n",
            "|UAE  |Fight  |\n",
            "|Sam  |NULL   |\n",
            "+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find the year, start_week_date, end_week_date, week_num**"
      ],
      "metadata": {
        "id": "HxRUbLW6njf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "data=[(2025,1,'2025-01-01'),\n",
        "      (2025,1,'2025-01-02'),\n",
        "      (2025,1,'2025-01-03'),\n",
        "      (2025,1,'2025-01-04'),\n",
        "      (2025,1,'2025-01-05'),\n",
        "      (2025,1,'2025-01-06'),\n",
        "      (2025,1,'2025-01-07'),\n",
        "      (2025,2,'2025-01-08'),\n",
        "      (2025,2,'2025-01-09'),\n",
        "      (2025,2,'2025-01-10'),\n",
        "      (2025,2,'2025-01-11'),\n",
        "      (2025,2,'2025-01-12'),\n",
        "      (2025,2,'2025-01-13'),\n",
        "      (2025,2,'2025-01-14')]\n",
        "\n",
        "schema=StructType([\n",
        "    StructField('year',IntegerType(),True),\n",
        "    StructField('week_num',IntegerType(),True),\n",
        "    StructField('dates',StringType(),True)\n",
        "    ])\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjwt_5cPnmKW",
        "outputId": "5bd54025-0ba8-439e-86f9-01241438f0ab"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+----------+\n",
            "|year|week_num|     dates|\n",
            "+----+--------+----------+\n",
            "|2025|       1|2025-01-01|\n",
            "|2025|       1|2025-01-02|\n",
            "|2025|       1|2025-01-03|\n",
            "|2025|       1|2025-01-04|\n",
            "|2025|       1|2025-01-05|\n",
            "|2025|       1|2025-01-06|\n",
            "|2025|       1|2025-01-07|\n",
            "|2025|       2|2025-01-08|\n",
            "|2025|       2|2025-01-09|\n",
            "|2025|       2|2025-01-10|\n",
            "|2025|       2|2025-01-11|\n",
            "|2025|       2|2025-01-12|\n",
            "|2025|       2|2025-01-13|\n",
            "|2025|       2|2025-01-14|\n",
            "+----+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "\n",
        "final_df = (\n",
        "    df.groupBy(\"year\", \"week_num\")\n",
        "      .agg(\n",
        "          min(\"dates\").alias(\"start_week_date\"),\n",
        "          max(\"dates\").alias(\"end_week_date\")\n",
        "      )\n",
        "      .orderBy(\"year\", \"week_num\")\n",
        ")\n",
        "final_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQjMfhSsoR48",
        "outputId": "bcb20eca-bc65-4efb-a216-cebfb8f3e1dd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+---------------+-------------+\n",
            "|year|week_num|start_week_date|end_week_date|\n",
            "+----+--------+---------------+-------------+\n",
            "|2025|       1|     2025-01-01|   2025-01-07|\n",
            "|2025|       2|     2025-01-08|   2025-01-14|\n",
            "+----+--------+---------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find the top most department having highest salary**"
      ],
      "metadata": {
        "id": "HVtLfUYbtH50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "schema = StructType([\n",
        "StructField(\"id\", IntegerType(), nullable=False),\n",
        "StructField(\"name\", StringType(), nullable=False),\n",
        "StructField(\"age\", IntegerType(), nullable=False),\n",
        "StructField(\"department\", StringType(), nullable=False),\n",
        "StructField(\"salary\", DoubleType(), nullable=False)\n",
        "])\n",
        "data = [\n",
        "    Row(1, \"John\", 30, \"Sales\", 50000.0),\n",
        "    Row(2, \"Alice\", 28, \"Marketing\", 60000.0),\n",
        "    Row(3, \"Bob\", 32, \"Finance\", 55000.0),\n",
        "    Row(4, \"Sarah\", 29, \"Sales\", 52000.0),\n",
        "    Row(5, \"Mike\", 31, \"Finance\", 58000.0)\n",
        "]\n",
        "employeeDF = spark.createDataFrame(data, schema)\n",
        "display(employeeDF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8O2B1HJYtKQL",
        "outputId": "c3b20db2-c586-4b4a-d7a0-0ed434dc0074"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: int, name: string, age: int, department: string, salary: double]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "# Group by department to get the top department\n",
        "top_dept_df = (\n",
        "employeeDF.groupBy(\"department\")\n",
        "                        .agg(sum(\"salary\").alias(\"total_salary\"))\n",
        "                        .orderBy(col(\"total_salary\").desc())\n",
        "                        .limit(1))\n",
        "\n",
        "top_dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAlCPbv4tUVj",
        "outputId": "10bbe67a-bc37-45f2-97c5-1baaac236b9c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+\n",
            "|department|total_salary|\n",
            "+----------+------------+\n",
            "|   Finance|    113000.0|\n",
            "+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find the origin and the destination flights for each customer**."
      ],
      "metadata": {
        "id": "_Esd4SjVuvnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import col, first, last\n",
        "\n",
        "# Create DataFrame\n",
        "flights_data = [\n",
        "    (1, 'Flight2', 'Los Angeles', 'London'),\n",
        "    (1, 'Flight1', 'London', 'Vatican'),\n",
        "    (1, 'Flight3', 'Vatican', 'Dubai'),\n",
        "    (2, 'Flight1', 'Houston', 'Paris'),\n",
        "    (2, 'Flight2', 'Paris', 'Hyderabad')\n",
        "]\n",
        "\n",
        "schema = \"cust_id int, flight_id string, origin string, destination string\"\n",
        "flights_df = spark.createDataFrame(flights_data, schema)\n",
        "\n",
        "# Show input\n",
        "flights_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr7GFbvkuyK6",
        "outputId": "4abe31a4-cf03-472c-b59a-d22da1722f15"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------+-----------+\n",
            "|cust_id|flight_id|     origin|destination|\n",
            "+-------+---------+-----------+-----------+\n",
            "|      1|  Flight2|Los Angeles|     London|\n",
            "|      1|  Flight1|     London|    Vatican|\n",
            "|      1|  Flight3|    Vatican|      Dubai|\n",
            "|      2|  Flight1|    Houston|      Paris|\n",
            "|      2|  Flight2|      Paris|  Hyderabad|\n",
            "+-------+---------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import first, last\n",
        "\n",
        "# Create a window partitioned by cust_id and ordered by flight_id\n",
        "w = Window.partitionBy(\"cust_id\").orderBy(\"flight_id\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "\n",
        "# Add columns for first origin and last destination\n",
        "result_df = (\n",
        "    flights_df.withColumn(\"origin_first\", first(\"origin\").over(w))\n",
        "    .withColumn(\"destination_last\", last(\"destination\").over(w))\n",
        "    .select(\"cust_id\", \"origin_first\", \"destination_last\")\n",
        "    .dropDuplicates()\n",
        ")\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQFPFyybzOBC",
        "outputId": "3065142d-95ff-4442-833f-6595f38ac99b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----------------+\n",
            "|cust_id|origin_first|destination_last|\n",
            "+-------+------------+----------------+\n",
            "|      1|      London|           Dubai|\n",
            "|      2|     Houston|       Hyderabad|\n",
            "+-------+------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write a PySpark program to select every 3rd (nth) row in the dataset**"
      ],
      "metadata": {
        "id": "V-3EWvEq-BIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "schema = StructType([\n",
        " StructField(\"emp_id\", IntegerType(), True),\n",
        " StructField(\"name\", StringType(), True),\n",
        " StructField(\"salary\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "data = [\n",
        " (1001, \"John Doe\", 50000),\n",
        " (2001, \"Jane Smith\", 60000),\n",
        " (1003, \"Michael Johnson\", 75000),\n",
        " (4000, \"Emily Davis\", 55000),\n",
        " (1005, \"Robert Brown\", 70000),\n",
        " (6000, \"Emma Wilson\", 80000),\n",
        " (1700, \"James Taylor\", 65000),\n",
        " (8000, \"Olivia Martinez\", 72000),\n",
        " (2900, \"William Anderson\", 68000),\n",
        " (3310, \"Sophia Garcia\", 67000)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "# df.display()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTOjox6L-DVQ",
        "outputId": "3956294a-6e7e-4563-9edc-b43fbeaa9ed4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------------+------+\n",
            "|emp_id|name            |salary|\n",
            "+------+----------------+------+\n",
            "|1001  |John Doe        |50000 |\n",
            "|2001  |Jane Smith      |60000 |\n",
            "|1003  |Michael Johnson |75000 |\n",
            "|4000  |Emily Davis     |55000 |\n",
            "|1005  |Robert Brown    |70000 |\n",
            "|6000  |Emma Wilson     |80000 |\n",
            "|1700  |James Taylor    |65000 |\n",
            "|8000  |Olivia Martinez |72000 |\n",
            "|2900  |William Anderson|68000 |\n",
            "|3310  |Sophia Garcia   |67000 |\n",
            "+------+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define window ordered by emp_id (or any stable column)\n",
        "window_spec = Window.orderBy(\"emp_id\")\n",
        "\n",
        "# Add row number and filter every 3rd row\n",
        "df_with_rn = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
        "result_df = df_with_rn.filter(col(\"row_num\") % 3 == 0).drop(\"row_num\")\n",
        "\n",
        "# Show result\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEsCra3P-5Km",
        "outputId": "b551cc3f-b9db-4206-f68b-aaee9e19adcb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------------+------+\n",
            "|emp_id|            name|salary|\n",
            "+------+----------------+------+\n",
            "|  1005|    Robert Brown| 70000|\n",
            "|  2900|William Anderson| 68000|\n",
            "|  6000|     Emma Wilson| 80000|\n",
            "+------+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle Data that has Error**"
      ],
      "metadata": {
        "id": "sVJjgZHPA8Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        " ('2025-01-01', '$100.50'),\n",
        " ('Jan 15, 2025', '€169.75'),\n",
        " ('03/20/25', '£150.20'),\n",
        " ('2025-05-13', '¥300.00')\n",
        "]\n",
        "columns = ['date', 'amount']\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import col, regexp_replace, udf, coalesce, to_date\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Define possible date formats\n",
        "date_formats = [\"yyyy-MM-dd\", \"MMM dd, yyyy\", \"MM/dd/yy\"]\n",
        "\n",
        "# Parse date column with multiple formats\n",
        "df_parsed = df.withColumn(\n",
        "    \"date_parsed\",\n",
        "    coalesce(\n",
        "        *[to_date(col(\"date\"), fmt) for fmt in date_formats]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Remove currency symbols from amount and convert to float\n",
        "df_cleaned = df_parsed.withColumn(\n",
        "    \"amount_num\",\n",
        "    regexp_replace(col(\"amount\"), r\"[^0-9.]\", \"\").cast(DoubleType())\n",
        ")\n",
        "\n",
        "df_cleaned.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6a9b4XlADCP",
        "outputId": "2eb724fe-60ac-4f42-c1c7-67518470baae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------+\n",
            "|date        |amount |\n",
            "+------------+-------+\n",
            "|2025-01-01  |$100.50|\n",
            "|Jan 15, 2025|€169.75|\n",
            "|03/20/25    |£150.20|\n",
            "|2025-05-13  |¥300.00|\n",
            "+------------+-------+\n",
            "\n",
            "+------------+-------+-----------+----------+\n",
            "|        date| amount|date_parsed|amount_num|\n",
            "+------------+-------+-----------+----------+\n",
            "|  2025-01-01|$100.50| 2025-01-01|     100.5|\n",
            "|Jan 15, 2025|€169.75| 2025-01-15|    169.75|\n",
            "|    03/20/25|£150.20| 2025-03-20|     150.2|\n",
            "|  2025-05-13|¥300.00| 2025-05-13|     300.0|\n",
            "+------------+-------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PIVOT in PySpark**\n",
        "\n",
        "agg(first(col2)) is necessary because:\n",
        "\n",
        "pivot() requires an aggregation function to combine values.\n",
        "first() is a simple aggregation that returns the single value you want when there is exactly one value per group.\n",
        "If you tried to do .pivot(\"col1\") without an aggregation, PySpark would raise an error because it needs to know how to combine multiple values."
      ],
      "metadata": {
        "id": "hYTKQWxjCZ7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import first\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, 'A', 10),\n",
        "    (1, 'B', 20),\n",
        "    (2, 'A', 30),\n",
        "    (2, 'B', 40)\n",
        "]\n",
        "columns = ['ID', 'col1', 'col2']\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Pivot the DataFrame\n",
        "pivot_df = df.groupBy(\"ID\").pivot(\"col1\").agg(first(\"col2\"))\n",
        "pivot_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pkpRDyaCFjH",
        "outputId": "9b3a6d1e-2cdc-401b-b377-0b38454e99a2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+\n",
            "| ID|  A|  B|\n",
            "+---+---+---+\n",
            "|  1| 10| 20|\n",
            "|  2| 30| 40|\n",
            "+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find the first non null phone number for the customer**"
      ],
      "metadata": {
        "id": "chHkwyrBDeCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(1, \"90909090900\", None, None),\n",
        " (2, None, \"8893029324903\", \"79767679988\"),\n",
        " (3, None, None, \"9090897576\"),\n",
        " (4, None, None, None)\n",
        "]\n",
        "\n",
        "columns = [\"customer_id\", \"home_phone\", \"mobile_phone\", \"work_phone\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5sYpf1gDiDW",
        "outputId": "4ac84215-e649-44ed-8a8d-f738fe6266a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-------------+-----------+\n",
            "|customer_id|home_phone |mobile_phone |work_phone |\n",
            "+-----------+-----------+-------------+-----------+\n",
            "|1          |90909090900|NULL         |NULL       |\n",
            "|2          |NULL       |8893029324903|79767679988|\n",
            "|3          |NULL       |NULL         |9090897576 |\n",
            "|4          |NULL       |NULL         |NULL       |\n",
            "+-----------+-----------+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import coalesce,col\n",
        "df_final=df.withColumn(\"first_not_null\",coalesce(col(\"home_phone\"),col(\"mobile_phone\"),col(\"work_phone\"))).drop(\"home_phone\",\"mobile_phone\",\"work_phone\")\n",
        "# display(df_final)\n",
        "df_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQRfUinNDvFl",
        "outputId": "5e152792-6b0e-45f0-a597-13d2c9bea8f3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|first_not_null|\n",
            "+-----------+--------------+\n",
            "|          1|   90909090900|\n",
            "|          2| 8893029324903|\n",
            "|          3|    9090897576|\n",
            "|          4|          NULL|\n",
            "+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group the data by department and find the employee with the highest salary in each department**"
      ],
      "metadata": {
        "id": "siGfV1ZeEoUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "schema = StructType([\n",
        "StructField(\"id\", IntegerType(), nullable=False),\n",
        "StructField(\"name\", StringType(), nullable=False),\n",
        "StructField(\"age\", IntegerType(), nullable=False),\n",
        "StructField(\"department\", StringType(), nullable=False),\n",
        "StructField(\"salary\", DoubleType(), nullable=False)\n",
        "])\n",
        "data = [\n",
        "    Row(1, \"Jonathan\", 30, \"Sales\", 50000.0),\n",
        "    Row(2, \"Jan\", 28, \"Marketing\", 60000.0),\n",
        "    Row(3, \"Steve\", 32, \"Finance\", 55000.0),\n",
        "    Row(4, \"Michelle\", 29, \"Sales\", 52000.0),\n",
        "]\n",
        "employeeDF = spark.createDataFrame(data, schema)\n",
        "employeeDF.show(truncate=False)\n",
        "# display(employeeDF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ8GN3-rErGW",
        "outputId": "6d7fe746-6724-45ae-ee5c-a9daab6a11cf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+---+----------+-------+\n",
            "|id |name    |age|department|salary |\n",
            "+---+--------+---+----------+-------+\n",
            "|1  |Jonathan|30 |Sales     |50000.0|\n",
            "|2  |Jan     |28 |Marketing |60000.0|\n",
            "|3  |Steve   |32 |Finance   |55000.0|\n",
            "|4  |Michelle|29 |Sales     |52000.0|\n",
            "+---+--------+---+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, row_number\n",
        "\n",
        "# Define the window specification\n",
        "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
        "\n",
        "# Add a row number within each department partition\n",
        "employee_with_rank = employeeDF.withColumn(\"rank\", row_number().over(windowSpec))\n",
        "\n",
        "# Filter to keep only the top salary per department\n",
        "top_earners = employee_with_rank.filter(col(\"rank\") == 1).drop(\"rank\")\n",
        "top_earners.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y1tzOy9FJ29",
        "outputId": "28b0d9c3-a0b7-46f1-b404-78682bcef943"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+---+----------+-------+\n",
            "| id|    name|age|department| salary|\n",
            "+---+--------+---+----------+-------+\n",
            "|  3|   Steve| 32|   Finance|55000.0|\n",
            "|  2|     Jan| 28| Marketing|60000.0|\n",
            "|  4|Michelle| 29|     Sales|52000.0|\n",
            "+---+--------+---+----------+-------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
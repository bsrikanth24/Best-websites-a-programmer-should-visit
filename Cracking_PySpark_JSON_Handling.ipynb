{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP99j3pY52e9ixX53xzEIWF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsrikanth24/Best-websites-a-programmer-should-visit/blob/master/Cracking_PySpark_JSON_Handling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.from_json()\n",
        "\n",
        "  This function parses a JSON string column into a PySpark StructType or other complex data types. It requires a schema to be specified."
      ],
      "metadata": {
        "id": "MGfOW3X8cvc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlKfXaAIb1bk",
        "outputId": "baacb294-bd79-4167-97f0-6770988b07aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+\n",
            "|    name|age|\n",
            "+--------+---+\n",
            "|Srikanth| 31|\n",
            "+--------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StructType, StringType, IntegerType\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"FromJSONExample\").getOrCreate()\n",
        "\n",
        "# Sample data (JSON string in DataFrame)\n",
        "data = [(\"\"\"{\"name\": \"Srikanth\", \"age\": 31}\"\"\",)]\n",
        "df = spark.createDataFrame(data, [\"json_data\"])\n",
        "\n",
        "# Define schema for JSON\n",
        "schema = StructType() \\\n",
        "    .add(\"name\", StringType(), True) \\\n",
        "    .add(\"age\", IntegerType(), True)\n",
        "\n",
        "# Parse JSON string into DataFrame columns\n",
        "df_parsed = df.withColumn(\"parsed_json\", from_json(col(\"json_data\"), schema))\n",
        "\n",
        "# Show result\n",
        "df_parsed.select(\"parsed_json.*\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **to_json()**\n",
        "\n",
        "    This function converts columns in a DataFrame into a JSON string. It works well for nested structures and complex data types."
      ],
      "metadata": {
        "id": "EXuX74okdS3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_json, struct\n",
        "\n",
        "# Sample data in a structured format\n",
        "data = [(\"Srikanth\", 31)]\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
        "\n",
        "# Convert structured columns into a JSON string\n",
        "df_json = df.withColumn(\"json_data\", to_json(struct(\"name\", \"age\")))\n",
        "\n",
        "# Show result\n",
        "df_json.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AazWx3idHeR",
        "outputId": "f9bdf418-1c9f-4282-d4bb-0c19b5cdd20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+----------------------------+\n",
            "|name    |age|json_data                   |\n",
            "+--------+---+----------------------------+\n",
            "|Srikanth|31 |{\"name\":\"Srikanth\",\"age\":31}|\n",
            "+--------+---+----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.json_tuple\n",
        "      \n",
        "  This function extracts multiple values from a JSON string based on the provided field names and creates new columns. It is useful for quickly extracting values from flat JSON strings."
      ],
      "metadata": {
        "id": "Zt4FvC5KdmXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import json_tuple\n",
        "\n",
        "# Sample data\n",
        "data = [(\"\"\"{\"name\": \"Srikanth\", \"age\": 31, \"city\": \"Hyderabad\"}\"\"\",)]\n",
        "df = spark.createDataFrame(data, [\"json_data\"])\n",
        "\n",
        "# Extract values using json_tuple\n",
        "df_extracted = df.select(json_tuple(col(\"json_data\"), \"name\", \"age\", \"city\") \\\n",
        "                  .alias(\"name\", \"age\", \"city\"))\n",
        "\n",
        "# Show result\n",
        "df_extracted.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMmksAtjde0F",
        "outputId": "e42d0ffc-4e0b-47ca-93aa-d77b454c0b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---------+\n",
            "|    name|age|     city|\n",
            "+--------+---+---------+\n",
            "|Srikanth| 31|Hyderabad|\n",
            "+--------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. get_json_object()\n",
        "\n",
        "  This function extracts a specific value from a JSON string based on the JSON path expression"
      ],
      "metadata": {
        "id": "BFK-S8QEeLwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import get_json_object\n",
        "\n",
        "# Sample data\n",
        "data = [(\"\"\"{\"name\": \"Srikanth\", \"age\": 31, \"city\": \"Hyderabad\"}\"\"\",)]\n",
        "df = spark.createDataFrame(data, [\"json_data\"])\n",
        "\n",
        "# Extract specific fields using JSON path\n",
        "df_extracted = df.withColumn(\"name\", get_json_object(col(\"json_data\"), \"$.name\")) \\\n",
        "                 .withColumn(\"city\", get_json_object(col(\"json_data\"), \"$.city\"))\n",
        "\n",
        "# Show result\n",
        "df_extracted.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8WV9Tg6eISs",
        "outputId": "26a4df52-e80c-41ff-b3a4-7dc679573b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------+--------+---------+\n",
            "|json_data                                           |name    |city     |\n",
            "+----------------------------------------------------+--------+---------+\n",
            "|{\"name\": \"Srikanth\", \"age\": 31, \"city\": \"Hyderabad\"}|Srikanth|Hyderabad|\n",
            "+----------------------------------------------------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. schema_of_json()\n",
        "\n",
        " This function infers the schema of a JSON string. It is useful when you don't know the schema beforehand and want to extract it dynamically."
      ],
      "metadata": {
        "id": "J-fDo8g5eopL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import schema_of_json\n",
        "\n",
        "# Sample JSON string\n",
        "json_string = '{\"name\": \"Srikanth\", \"age\": 31}'\n",
        "\n",
        "# Infer schema from the JSON string\n",
        "schema = spark.range(1) \\\n",
        "        .select(schema_of_json(json_string) \\\n",
        "        .alias(\"schema\")).collect()[0][0]\n",
        "\n",
        "# Print schema\n",
        "print(schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odhO0zkdeiso",
        "outputId": "d813308d-d473-477a-c3ff-99b45eb5652d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STRUCT<age: BIGINT, name: STRING>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:\n",
        "\n",
        "In PySpark, handling nested JSON data involves working with complex data types such as `ArrayType`, `MapType`, and `StructType`. Here's an example of how to process a nested JSON structure that includes these data types. We will:\n",
        "\n",
        "1. Define a schema with `StructType`, `ArrayType`, and `MapType`. 2. Read the JSON data. 3. Access and manipulate the nested fields.\n",
        "\n",
        "**Step 1: Create a Spark session**"
      ],
      "metadata": {
        "id": "y59KcUSFe7Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "# This session is required for any PySpark code to run\n",
        "spark = SparkSession.builder.appName(\"JSONProcessing\").getOrCreate()"
      ],
      "metadata": {
        "id": "ZEfu5gpYfD2d"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define sample data"
      ],
      "metadata": {
        "id": "2kl4SdAqf17m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The sample data contains a single row with nested JSON structure.\n",
        "# data = [(\n",
        "#     '''{\n",
        "#         \"user_id\": \"U123\",\n",
        "#         \"info\": {\n",
        "#             \"name\": \"Srikanth\",\n",
        "#             \"age\": 31,\n",
        "#             \"preferences\": {\n",
        "#                 \"colors\": [\"blue\", \"green\"],\n",
        "#                 \"hobbies\": [\"reading\", \"traveling\"]\n",
        "#             }\n",
        "#         },\n",
        "#         \"addresses\": [\n",
        "#             {\n",
        "#                 \"type\": \"home\",\n",
        "#                 \"city\": \"Hyderabad\",\n",
        "#                 \"postalCode\": \"500048\"\n",
        "#             },\n",
        "#             {\n",
        "#                 \"type\": \"office\",\n",
        "#                 \"city\": \"Bangalore\",\n",
        "#                 \"postalCode\": \"560001\"\n",
        "#             }\n",
        "#         ],\n",
        "#         \"metadata\": {\n",
        "#             \"likes\": 100,\n",
        "#             \"social\": {\n",
        "#                 \"twitter\": \"@SmartInvGuide\",\n",
        "#                 \"linkedin\": \"http://www.linkedin.com/in/srikanthb24/ \"\n",
        "#             }\n",
        "#         }\n",
        "#     }'''\n",
        "# ,)]\n",
        "\n",
        "\n",
        "data = [(\n",
        "    '''{\n",
        "    \"user_id\": \"12345\",\n",
        "    \"info\": {\n",
        "        \"name\": \"Srikanth\",\n",
        "        \"age\": 30,\n",
        "        \"preferences\": {\n",
        "            \"colors\": [\"red\", \"blue\"],\n",
        "            \"hobbies\": [\"reading\", \"gaming\"]\n",
        "        }\n",
        "    },\n",
        "    \"addresses\": [\n",
        "        {\n",
        "            \"type\": \"home\",\n",
        "            \"city\": \"Hyderabad\",\n",
        "            \"postalCode\": \"500048\"\n",
        "        }\n",
        "    ],\n",
        "    \"metadata\": {\n",
        "        \"likes\": 100,\n",
        "        \"social\": {\n",
        "            \"twitter\": \"@srikanth9\",\n",
        "            \"linkedin\": \"srikanth24\"\n",
        "        }\n",
        "    }\n",
        "}'''\n",
        ",)]"
      ],
      "metadata": {
        "id": "AZGQVs7ef2tx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create a DataFrame with the JSON string data"
      ],
      "metadata": {
        "id": "yNO3ajyLgl7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We're loading the raw JSON data as a single string in a DataFrame\n",
        "df = spark.createDataFrame(data, [\"json_data\"])"
      ],
      "metadata": {
        "id": "mIuCqP5zgmvz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Parse the JSON string"
      ],
      "metadata": {
        "id": "OAFfjNzUgrti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
        "\n",
        "# Define the schema for the JSON data\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"info\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"age\", IntegerType(), True),\n",
        "        StructField(\"preferences\", StructType([\n",
        "            StructField(\"colors\", ArrayType(StringType()), True),\n",
        "            StructField(\"hobbies\", ArrayType(StringType()), True)\n",
        "        ]), True)\n",
        "    ]), True),\n",
        "    StructField(\"addresses\", ArrayType(StructType([\n",
        "        StructField(\"type\", StringType(), True),\n",
        "        StructField(\"city\", StringType(), True),\n",
        "        StructField(\"postalCode\", StringType(), True)\n",
        "    ])), True),\n",
        "    StructField(\"metadata\", StructType([\n",
        "        StructField(\"likes\", IntegerType(), True),\n",
        "        StructField(\"social\", StructType([\n",
        "            StructField(\"twitter\", StringType(), True),\n",
        "            StructField(\"linkedin\", StringType(), True)\n",
        "        ]), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# Convert JSON string to DataFrame with structured columns\n",
        "json_df = df.selectExpr(\"CAST(json_data AS STRING) as json\") \\\n",
        "    .select(from_json(\"json\", schema).alias(\"data\"))"
      ],
      "metadata": {
        "id": "MlM0fyspgsdf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Flatten the DataFrame"
      ],
      "metadata": {
        "id": "_KKZrTDIg8DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the nested fields using dot notation\n",
        "# We also explode the 'addresses' array to create separate rows for each address entry.\n",
        "flattened_df = json_df.select(\n",
        "    col(\"data.user_id\"),\n",
        "    col(\"data.info.name\"),\n",
        "    col(\"data.info.age\"),\n",
        "    col(\"data.info.preferences.colors\"),              # Preferred colors (array)\n",
        "    col(\"data.info.preferences.hobbies\"),             # Hobbies (array)\n",
        "    explode(col(\"data.addresses\")).alias(\"address\"),  # Explode addresses array into rows\n",
        "    col(\"data.metadata.likes\"),\n",
        "    col(\"data.metadata.social.twitter\"),\n",
        "    col(\"data.metadata.social.linkedin\")\n",
        ")"
      ],
      "metadata": {
        "id": "cs4a-wHcg8tl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HzflJ_hPjWbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Select and rename relevant fields"
      ],
      "metadata": {
        "id": "fWlUmtyKi5Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After exploding 'addresses', select the individual fields of each address and rename them.\n",
        "result_df = flattened_df.select(\n",
        "    \"user_id\",\n",
        "    \"name\",\n",
        "    \"age\",\n",
        "    \"colors\",\n",
        "    \"hobbies\",\n",
        "    col(\"address.type\").alias(\"address_type\"),  # Address type (home/office)\n",
        "    col(\"address.city\").alias(\"address_city\"),\n",
        "    col(\"address.postalCode\").alias(\"address_postal_code\"),\n",
        "    \"likes\",\n",
        "    \"twitter\",\n",
        "    \"linkedin\"\n",
        ")"
      ],
      "metadata": {
        "id": "RWTQqyzDi5wN"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Display the DataFrame"
      ],
      "metadata": {
        "id": "76QBO-HPi_N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the resulting flattened DataFrame with all selected fields.\n",
        "result_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgKEbGXSi_oP",
        "outputId": "1943687d-2266-4605-eb24-fb1ee948e276"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+---+-----------+-----------------+------------+------------+-------------------+-----+----------+----------+\n",
            "|user_id|name    |age|colors     |hobbies          |address_type|address_city|address_postal_code|likes|twitter   |linkedin  |\n",
            "+-------+--------+---+-----------+-----------------+------------+------------+-------------------+-----+----------+----------+\n",
            "|12345  |Srikanth|30 |[red, blue]|[reading, gaming]|home        |Hyderabad   |500048             |100  |@srikanth9|srikanth24|\n",
            "+-------+--------+---+-----------+-----------------+------------+------------+-------------------+-----+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read JSON multiple lines**\n",
        "\n",
        "To read a multi-line JSON file in PySpark, you can use the `multiline` option while reading the JSON."
      ],
      "metadata": {
        "id": "DT9TtNMhjxZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"ReadMultilineJSON\").getOrCreate()\n",
        "\n",
        "# Path to your JSON file\n",
        "json_file_path = \"/path/multiline_file.json\"\n",
        "\n",
        "# Read multi-line JSON file\n",
        "df = spark.read.option(\"multiline\", \"true\").json(json_file_path)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "zKbkgNvvjzmI",
        "outputId": "522342d0-3dee-418a-ad75-03268d98ca69"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/path/multiline_file.json.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-e378b1d49481>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Read multi-line JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multiline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Show DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/path/multiline_file.json."
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdDSwsLvbdOmt5jqY06oEb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsrikanth24/Best-websites-a-programmer-should-visit/blob/master/Convert_a_string_column_in_a_pyspark_dataframe_to_List_of_Dictionaries%3F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-rNV2Q2DlvGH"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    {\n",
        "    \"employee_id\": 873547690,\n",
        "    \"employee_name\":\"abc\",\n",
        "    \"emp_deptname\": \"sales\",\n",
        "    \"emp_endpoint\": \"abc@abc01storage\",\n",
        "    \"emp_folder\": \"messages\",\n",
        "    \"emp_address\": [\n",
        "                {\n",
        "                    \"type\": \"Home\",\n",
        "                    \"city\": \"Delhi\",\n",
        "                    \"apartment_number\": \"H.Number 124, B block, XYZ Towers\",\n",
        "                    \"pincode\": \"123abc\"\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"Residential\",\n",
        "                    \"city\": \"Delhi\",\n",
        "                    \"apartment_number\": \"Post Office 12A, Sector 22\",\n",
        "                    \"pincode\": \"456xyz\"\n",
        "                }\n",
        "            ],\n",
        "    'event_timestamp': '1995-07-16 13:10:43'\n",
        "        },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"employee_id long, employee_name string, emp_deptname string, emp_endpoint string, emp_folder string, emp_address string, event_timestamp string\""
      ],
      "metadata": {
        "id": "2f0DjGjal-F4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema_str = \"\"\"\n",
        "    employee_id long,\n",
        "    employee_name string,\n",
        "    emp_deptname string,\n",
        "    emp_endpoint string,\n",
        "    emp_folder string,\n",
        "    emp_address array<struct<type string, city string, apartment_number string, pincode string>>,\n",
        "    event_timestamp string\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j71DOD26nml_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01107309"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sdf = spark.createDataFrame(data, schema_str)\n",
        "\n",
        "data_sdf.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXr1gHHzm_SA",
        "outputId": "7cdf63a1-84ba-46a1-eb1a-8e193cb0ca78"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+------------+----------------+----------+--------------------------------------------------------------------------------------------------------------------+-------------------+\n",
            "|employee_id|employee_name|emp_deptname|emp_endpoint    |emp_folder|emp_address                                                                                                         |event_timestamp    |\n",
            "+-----------+-------------+------------+----------------+----------+--------------------------------------------------------------------------------------------------------------------+-------------------+\n",
            "|873547690  |abc          |sales       |abc@abc01storage|messages  |[{Home, Delhi, H.Number 124, B block, XYZ Towers, 123abc}, {Residential, Delhi, Post Office 12A, Sector 22, 456xyz}]|1995-07-16 13:10:43|\n",
            "+-----------+-------------+------------+----------------+----------+--------------------------------------------------------------------------------------------------------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_sdf.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvU_azxXnYv_",
        "outputId": "62ec27dc-6b7c-4847-c2af-08df292421e4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: long (nullable = true)\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- emp_deptname: string (nullable = true)\n",
            " |-- emp_endpoint: string (nullable = true)\n",
            " |-- emp_folder: string (nullable = true)\n",
            " |-- emp_address: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- type: string (nullable = true)\n",
            " |    |    |-- city: string (nullable = true)\n",
            " |    |    |-- apartment_number: string (nullable = true)\n",
            " |    |    |-- pincode: string (nullable = true)\n",
            " |-- event_timestamp: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_sdf.select(\"emp_address\").schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPr0T7FhoTSX",
        "outputId": "c98cdda4-3c6b-480d-e69f-e17c35ff10ef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('emp_address', ArrayType(StructType([StructField('type', StringType(), True), StructField('city', StringType(), True), StructField('apartment_number', StringType(), True), StructField('pincode', StringType(), True)]), True), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_address = data_sdf.select(\"emp_address\").collect()\n",
        "list(emp_address[0][\"emp_address\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhvzHAJIoORv",
        "outputId": "a405a287-b873-4f17-d0f6-c9c9ba941677"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(type='Home', city='Delhi', apartment_number='H.Number 124, B block, XYZ Towers', pincode='123abc'),\n",
              " Row(type='Residential', city='Delhi', apartment_number='Post Office 12A, Sector 22', pincode='456xyz')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Input DF\n",
        "    # +-----+-------+\n",
        "    # |group|subject|\n",
        "    # +-----+-------+\n",
        "    # |    A|   Math|\n",
        "    # |    A|Physics|\n",
        "    # |    B|Science|\n",
        "    # +-----+-------+"
      ],
      "metadata": {
        "id": "e9MGJzvuurAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05857141",
        "outputId": "0b944cf4-0535-4eb2-e54b-e21bf1afd614"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data_input = [Row(group='A', subject='Math'),\n",
        "              Row(group='A', subject='Physics'),\n",
        "              Row(group='B', subject='Science')]\n",
        "\n",
        "df1 = spark.createDataFrame(data_input)\n",
        "\n",
        "df1.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+\n",
            "|group|subject|\n",
            "+-----+-------+\n",
            "|    A|   Math|\n",
            "|    A|Physics|\n",
            "|    B|Science|\n",
            "+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Row\n",
        "\n",
        "df1 = df1.groupBy(\"group\").agg(F.collect_list(\"subject\").alias(\"subject\")).orderBy(\"group\")\n",
        "df1.show(truncate=False)\n",
        "\n",
        "dict = {row['group']:row['subject'] for row in df1.collect()}\n",
        "print(dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hurqg9ihu2QF",
        "outputId": "010f76e3-1b2e-4822-b03a-4fd13cc990af"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------+\n",
            "|group|subject        |\n",
            "+-----+---------------+\n",
            "|A    |[Math, Physics]|\n",
            "|B    |[Science]      |\n",
            "+-----+---------------+\n",
            "\n",
            "{'A': ['Math', 'Physics'], 'B': ['Science']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "df= sc.parallelize([(1,2,3), (4,5,7)]).toDF([\"a\", \"b\", \"c\"])\n",
        "if df is not None and isinstance(df,DataFrame):\n",
        "\n",
        "      print(\"dataframe exists\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "43pj_W1rwDP3",
        "outputId": "755668d2-9ff9-4dc4-e2d9-3132e8f38e42"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2494318389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataframe exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario 1:\n",
        "\n",
        "Input data:\n",
        "\n",
        "col1|col2|date\n",
        "100|Austin|2021-01-10\n",
        "100|Newyork|2021-02-15\n",
        "100|Austin|2021-03-02\n",
        "Expected output with CDC:\n",
        "\n",
        "col1|col2|start_date|end_date\n",
        "100|Austin|2021-01-10|2021-02-15\n",
        "100|Newyork|2021-02-15|2021-03-02\n",
        "100|Austin|2021-03-02|2099-12-31\n",
        "In sequence there is a change in col2 values and want to maintain CDC"
      ],
      "metadata": {
        "id": "pqcq6BX2105M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"SO\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(100, \"Austin\", \"2021-01-10\"),\n",
        "(100, \"Newyork\", \"2021-02-15\"),\n",
        "(100, \"Austin\", \"2021-03-02\"),\n",
        "    ],\n",
        "    ['col1', 'col2', 'date']\n",
        ")\n",
        "\n",
        "df.show()\n",
        "\n",
        "df1 = df.withColumn(\"start_date\", F.to_date(\"date\"))\n",
        "\n",
        "w = Window.partitionBy(\"col1\",).orderBy(\"start_date\")\n",
        "\n",
        "df_1 = df1.withColumn(\"rn\", F.row_number().over(w))\n",
        "\n",
        "df_1.createOrReplaceTempView(\"temp\")\n",
        "\n",
        "df_dupe = spark.sql('select temp.col1,temp.col2,temp.start_date, case when temp.col1=temp_2.col1 and temp.col2=temp_2.col2 then \"delete\" else \"no-delete\" end as dupe  from temp left join temp as temp_2 '\n",
        "                    'on temp.col1=temp_2.col1 and temp.col2=temp_2.col2 and temp.rn-1 = temp_2.rn order by temp.start_date  ')\n",
        "\n",
        "df_dupe.filter(F.col(\"dupe\")==\"no-delete\").drop(\"dupe\")\\\n",
        "    .withColumn(\"end_date\", F.coalesce(F.lead(\"start_date\").over(w),F.lit(\"2099-12-31\"))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1JHy_wRxkxk",
        "outputId": "8219e940-6628-4339-fd38-c111701e6dbf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+----------+\n",
            "|col1|   col2|      date|\n",
            "+----+-------+----------+\n",
            "| 100| Austin|2021-01-10|\n",
            "| 100|Newyork|2021-02-15|\n",
            "| 100| Austin|2021-03-02|\n",
            "+----+-------+----------+\n",
            "\n",
            "+----+-------+----------+----------+\n",
            "|col1|   col2|start_date|  end_date|\n",
            "+----+-------+----------+----------+\n",
            "| 100| Austin|2021-01-10|2021-02-15|\n",
            "| 100|Newyork|2021-02-15|2021-03-02|\n",
            "| 100| Austin|2021-03-02|2099-12-31|\n",
            "+----+-------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario 2:\n",
        "\n",
        "Input:\n",
        "\n",
        "col1|col2|date\n",
        "100|Austin|2021-01-10\n",
        "100|Austin|2021-03-02  -> I want to eliminate this version because there is no change in col1 and col2 values between records.\n",
        "Expected Output:\n",
        "\n",
        " col1|col2|start_date|end_date\n",
        " 100|Austin|2021-01-10|2099-12-31"
      ],
      "metadata": {
        "id": "-FxUkM661acV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CDC_Example\").getOrCreate()\n",
        "\n",
        "data = spark.createDataFrame(\n",
        "    [(100, \"Austin\", \"2021-01-10\"),\n",
        "   (100, \"Austin\", \"2021-03-02\")],\n",
        "    ['col1', 'col2', 'date']\n",
        ")\n",
        "df = data\n",
        "\n",
        "# Convert date string to date type\n",
        "df = df.withColumn(\"date\", F.to_date(\"date\"))\n",
        "\n",
        "# Define window partitioned by col1 ordered by date\n",
        "w = Window.partitionBy(\"col1\").orderBy(\"date\")\n",
        "\n",
        "# Get previous col2 value\n",
        "df = df.withColumn(\"prev_col2\", F.lag(\"col2\").over(w))\n",
        "\n",
        "# Filter rows where col2 != prev_col2 or prev_col2 is null (first record)\n",
        "df_filtered = df.filter((F.col(\"col2\") != F.col(\"prev_col2\")) | F.col(\"prev_col2\").isNull())\n",
        "\n",
        "# Add start_date and end_date columns\n",
        "df_final = df_filtered.withColumnRenamed(\"date\", \"start_date\") \\\n",
        "                      .withColumn(\"end_date\", F.lit(\"2099-12-31\")) \\\n",
        "                      .select(\"col1\", \"col2\", \"start_date\", \"end_date\")\n",
        "\n",
        "df_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqtlvlOWz5VW",
        "outputId": "929312ba-cdb9-4b8a-dffe-45c463f6215b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+----------+----------+\n",
            "|col1|  col2|start_date|  end_date|\n",
            "+----+------+----------+----------+\n",
            "| 100|Austin|2021-01-10|2099-12-31|\n",
            "+----+------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "InputDF\n",
        "    # +-------------+----------+\n",
        "    # |UNIQUE_MEM_ID|      DATE|\n",
        "    # +-------------+----------+\n",
        "    # |         1156|      null|\n",
        "    # |         3787|2016-07-05|\n",
        "    # |         1156|      null|\n",
        "    # +-------------+----------+"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "kdxn8aIl7S6b",
        "outputId": "5fff6c7e-e9a2-4579-ee80-d200dc5f7e25"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'InputDF' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-723855443.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mInputDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# +-------------+----------+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# |UNIQUE_MEM_ID|      DATE|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# +-------------+----------+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# |         1156|      null|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'InputDF' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05f0fb2d",
        "outputId": "a3d5599e-0cb5-4674-9f1f-bd3295318728"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Ensure a SparkSession is available\n",
        "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
        "\n",
        "\n",
        "data_input_df = [Row(UNIQUE_MEM_ID=1156, DATE=None),\n",
        "                 Row(UNIQUE_MEM_ID=3787, DATE='2016-07-05'),\n",
        "                 Row(UNIQUE_MEM_ID=1156, DATE=None)]\n",
        "\n",
        "InputDF = spark.createDataFrame(data_input_df)\n",
        "\n",
        "InputDF.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+\n",
            "|UNIQUE_MEM_ID|      DATE|\n",
            "+-------------+----------+\n",
            "|         1156|      NULL|\n",
            "|         3787|2016-07-05|\n",
            "|         1156|      NULL|\n",
            "+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "InputDF.select( *[ F.when(F.col(column).isNull(),'').otherwise(F.col(column)).alias(column) for column in InputDF.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19XzNkuw7eRb",
        "outputId": "863d24e4-66a4-4614-92f3-f749633eeaff"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+\n",
            "|UNIQUE_MEM_ID|      DATE|\n",
            "+-------------+----------+\n",
            "|         1156|          |\n",
            "|         3787|2016-07-05|\n",
            "|         1156|          |\n",
            "+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}